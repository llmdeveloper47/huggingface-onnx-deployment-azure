{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cce326a4-fe8b-4c24-b735-5e483a9de180",
   "metadata": {},
   "source": [
    "# Dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8742aa36-9c56-40a0-9aed-07a7e3f60461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch\n",
    "# !pip install transformers\n",
    "# !pip install azureml azureml.core\n",
    "# !pip install onnxruntime\n",
    "# !pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e2a728-5664-4cd9-887b-08c26368c25f",
   "metadata": {},
   "source": [
    "# Export Model as ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53af36b9-f163-4218-8d0e-ada00d393ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/onnxruntime/training/utils/hooks/_zero_offload_subscriber.py:173: UserWarning: DeepSpeed import error No module named 'deepspeed'\n",
      "  warnings.warn(f\"DeepSpeed import error {e}\")\n",
      "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:114: UserWarning: WARNING: failed to get cudart_version from onnxruntime build info.\n",
      "  warnings.warn(\"WARNING: failed to get cudart_version from onnxruntime build info.\")\n",
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForQuestionAnswering\n",
    "\n",
    "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "model_path = \"./\" + model_name + \".onnx\"\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# set the model to inference mode\n",
    "# It is important to call torch_model.eval() or torch_model.train(False) before exporting the model, \n",
    "# to turn the model to inference mode. This is required since operators like dropout or batchnorm \n",
    "# behave differently in inference and training mode.\n",
    "model.eval()\n",
    "\n",
    "# Generate dummy inputs to the model. Adjust if neccessary\n",
    "inputs = {\n",
    "        'input_ids':   torch.randint(32, [1, 32], dtype=torch.long), # list of numerical ids for the tokenized text\n",
    "        'attention_mask': torch.ones([1, 32], dtype=torch.long),     # dummy list of ones\n",
    "        'token_type_ids':  torch.ones([1, 32], dtype=torch.long)     # dummy list of ones\n",
    "    }\n",
    "\n",
    "symbolic_names = {0: 'batch_size', 1: 'max_seq_len'}\n",
    "torch.onnx.export(model,                                         # model being run\n",
    "                  (inputs['input_ids'],\n",
    "                   inputs['attention_mask'], \n",
    "                   inputs['token_type_ids']),                    # model input (or a tuple for multiple inputs)\n",
    "                  model_path,                                    # where to save the model (can be a file or file-like object)\n",
    "                  opset_version=11,                              # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,                      # whether to execute constant folding for optimization\n",
    "                  input_names=['input_ids',\n",
    "                               'input_mask', \n",
    "                               'segment_ids'],                   # the model's input names\n",
    "                  output_names=['start_logits', \"end_logits\"],   # the model's output names\n",
    "                  dynamic_axes={'input_ids': symbolic_names,\n",
    "                                'input_mask' : symbolic_names,\n",
    "                                'segment_ids' : symbolic_names,\n",
    "                                'start_logits' : symbolic_names, \n",
    "                                'end_logits': symbolic_names})   # variable length axes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e06a9d2-3f35-4123-a8df-895ae7d2f5a3",
   "metadata": {},
   "source": [
    "# Inference Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "487335a4-e35c-4416-a753-e3f3c63226ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'Rebecca'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "import numpy as np\n",
    "import onnxruntime\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "# The pre process function take a question and a context, and generates the tensor inputs to the model:\n",
    "# - input_ids: the words in the question encoded as integers\n",
    "# - attention_mask: not used in this model\n",
    "# - token_type_ids: a list of 0s and 1s that distinguish between the words of the question and the words of the context\n",
    "# This function also returns the words contained in the question and the context, so that the answer can be decoded into a phrase. \n",
    "def preprocess(question, context):\n",
    "    encoded_input = tokenizer(question, context)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoded_input.input_ids)\n",
    "    return (encoded_input.input_ids, encoded_input.attention_mask, encoded_input.token_type_ids, tokens)\n",
    "\n",
    "# The post process function maps the list of start and end log probabilities onto a text answer, using the text tokens from the question\n",
    "# and context. \n",
    "def postprocess(tokens, start, end):\n",
    "    results = {}\n",
    "    answer_start = np.argmax(start)\n",
    "    answer_end = np.argmax(end)\n",
    "    if answer_end >= answer_start:\n",
    "        answer = tokens[answer_start]\n",
    "        for i in range(answer_start+1, answer_end+1):\n",
    "            if tokens[i][0:2] == \"##\":\n",
    "                answer += tokens[i][2:]\n",
    "            else:\n",
    "                answer += \" \" + tokens[i]\n",
    "        results['answer'] = answer.capitalize()\n",
    "    else:\n",
    "        results['error'] = \"I am unable to find the answer to this question. Can you please ask another question?\"\n",
    "    return results\n",
    "\n",
    "# Perform the one-off initialization for the prediction. The init code is run once when the endpoint is setup.\n",
    "def init():\n",
    "    global tokenizer, session, model\n",
    "\n",
    "    model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "    model = transformers.BertForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "    # use AZUREML_MODEL_DIR to get your deployed model(s). If multiple models are deployed, \n",
    "    # model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), '$MODEL_NAME/$VERSION/$MODEL_FILE_NAME')\n",
    "    model_dir = os.getenv('AZUREML_MODEL_DIR')\n",
    "    if model_dir == None:\n",
    "        model_dir = \"./\"\n",
    "    model_path = os.path.join(model_dir, model_name + \".onnx\")\n",
    "\n",
    "    # Create the tokenizer\n",
    "    tokenizer = transformers.BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Create an ONNX Runtime session to run the ONNX model\n",
    "    session = onnxruntime.InferenceSession(model_path, providers=[\"CPUExecutionProvider\"])  \n",
    "\n",
    "\n",
    "# Run the PyTorch model, for functional and performance comparison\n",
    "def run_pytorch(raw_data):\n",
    "    inputs = json.loads(raw_data)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    logging.info(\"Question:\", inputs[\"question\"])\n",
    "    logging.info(\"Context: \", inputs[\"context\"])\n",
    "\n",
    "    input_ids, input_mask, segment_ids, tokens = preprocess(inputs[\"question\"], inputs[\"context\"])\n",
    "    model_outputs = model(torch.tensor([input_ids]),  token_type_ids=torch.tensor([segment_ids]))\n",
    "    return postprocess(tokens, model_outputs.start_logits.detach().numpy(), model_outputs.end_logits.detach().numpy())\n",
    "\n",
    "# Run the ONNX model with ONNX Runtime\n",
    "def run(raw_data):\n",
    "    logging.info(\"Request received\")\n",
    "    inputs = json.loads(raw_data)\n",
    "    logging.info(inputs)\n",
    "\n",
    "    # Preprocess the question and context into tokenized ids\n",
    "    input_ids, input_mask, segment_ids, tokens = preprocess(inputs[\"question\"], inputs[\"context\"])\n",
    "    logging.info(\"Running inference\")\n",
    "    \n",
    "    # Format the inputs for ONNX Runtime\n",
    "    model_inputs = {\n",
    "        'input_ids':   [input_ids], \n",
    "        'input_mask':  [input_mask],\n",
    "        'segment_ids': [segment_ids]\n",
    "        }\n",
    "                  \n",
    "    outputs = session.run(['start_logits', 'end_logits'], model_inputs)\n",
    "    logging.info(\"Post-processing\")  \n",
    "\n",
    "    # Post process the output of the model into an answer (or an error if the question could not be answered)\n",
    "    results = postprocess(tokens, outputs[0], outputs[1])\n",
    "    logging.info(results)\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    init()\n",
    "\n",
    "    input = \"{\\\"question\\\": \\\"What is Dolly Parton's middle name?\\\", \\\"context\\\": \\\"Dolly Rebecca Parton is an American singer-songwriter\\\"}\"\n",
    "\n",
    "    run_pytorch(input)\n",
    "    print(run(input))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815dfd0c-7f53-4334-9bf9-c36dcbdda7d3",
   "metadata": {},
   "source": [
    "# Check Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52b70559-4831-405f-a319-d1b1c96a8e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version:  4.38.2\n",
      "Torch (ONNX exporter) version:  2.2.1+cu121\n",
      "Azure SDK version: 1.51.0\n",
      "ONNX Runtime version:  1.17.1\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "import onnxruntime\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "print(\"Transformers version: \", transformers.__version__)\n",
    "torch_version = torch.__version__\n",
    "print(\"Torch (ONNX exporter) version: \", torch_version)\n",
    "print(\"Azure SDK version:\", azureml.core.VERSION)\n",
    "print(\"ONNX Runtime version: \", onnxruntime.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c03056-c2e7-4882-97ae-e9f28e9ef84e",
   "metadata": {},
   "source": [
    "# Register Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e465b76-d4f5-4073-8953-e03783f9f3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aml_worspace\n",
      "eastus2\n",
      "machine-learning\n",
      "<subscription_ids>\n",
      "Registering model bert-large-uncased-whole-word-masking-finetuned-squad\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.location, ws.resource_group, ws.subscription_id, sep = '\\n')\n",
    "#Register your model with Azure ML\n",
    "#Now we upload the model and register it in the workspace.\n",
    "\n",
    "from azureml.core.model import Model\n",
    "\n",
    "model = Model.register(model_path = model_path,                 # Name of the registered model in your workspace.\n",
    "                       model_name = model_name,            # Local ONNX model to upload and register as a model\n",
    "                       model_framework=Model.Framework.ONNX ,   # Framework used to create the model.\n",
    "                       model_framework_version=torch_version,   # Version of ONNX used to create the model.\n",
    "                       tags = {\"onnx\": \"demo\"},\n",
    "                       description = \"HuggingFace BERT model fine-tuned with SQuAd and exported from PyTorch\",\n",
    "                       workspace = ws)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe65012-b662-421d-a212-6ed3c4f2b493",
   "metadata": {},
   "source": [
    "# Check Registered Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89180bee-5cdf-4e3a-b9ab-b524ecf91877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: gpt2 \tVersion: 1 \tDescription: gpt2 model from OpenAI {}\n",
      "Name: bge-large-onnx \tVersion: 1 \tDescription: None {}\n",
      "Name: bert \tVersion: 1 \tDescription: None {}\n",
      "Name: bert-large-uncased-whole-word-masking-finetuned-squad \tVersion: 1 \tDescription: HuggingFace BERT model fine-tuned with SQuAd and exported from PyTorch {'onnx': 'demo'}\n"
     ]
    }
   ],
   "source": [
    "models = ws.models\n",
    "for name, m in models.items():\n",
    "    print(\"Name:\", name,\"\\tVersion:\", m.version, \"\\tDescription:\", m.description, m.tags)\n",
    "    \n",
    "#     # If you'd like to delete the models from workspace\n",
    "#     model_to_delete = Model(ws, name)\n",
    "#     model_to_delete.delete()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ae6a70-c94b-4ae9-8da0-a0b13d7f5a8b",
   "metadata": {},
   "source": [
    "# Deploy Endpoint\n",
    "\n",
    " - Make a custom environment\n",
    " - Go to Registed Model, and use the above deployment script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b2e5d9d-0d47-4d0b-89c9-f40e6e14c1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dependencies_dir = \"./dependencies\"\n",
    "os.makedirs(dependencies_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc11b38a-2eee-4271-bbc6-6a882503c372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./dependencies/conda.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {dependencies_dir}/conda.yml\n",
    "name: project_environment\n",
    "dependencies:\n",
    "  - python=3.8\n",
    "  - pip:\n",
    "      - numpy\n",
    "      - onnxruntime\n",
    "      - transformers\n",
    "      - torch\n",
    "      - azureml-core~=1.40.0\n",
    "      - azureml-defaults~=1.40.0\n",
    "      - socks\n",
    "channels:\n",
    "  - anaconda\n",
    "  - conda-forge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcae7c6a-8b5e-4a26-ac15-0e3df6697c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.ai.ml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "063ea9b9-a798-48d8-83e7-fa6c4f6b7711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml import command, Input\n",
    "from azure.ai.ml.entities import (\n",
    "    AzureBlobDatastore,\n",
    "    AzureFileDatastore,\n",
    "    AzureDataLakeGen1Datastore,\n",
    "    AzureDataLakeGen2Datastore,\n",
    ")\n",
    "from azure.ai.ml.entities import Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d947a1b-9228-4d78-b211-ad843fb40aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = \"9f63dff7-d2e3-42cc-bf0c-c2be977a6c22\"\n",
    "resource_group = \"machine-learning\"\n",
    "workspace = \"AML_Worspace\"\n",
    "\n",
    "ml_client = MLClient(\n",
    "    DefaultAzureCredential(), subscription_id, resource_group, workspace\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76f8f65-3246-4671-9cf4-ab73bd97134b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "\n",
    "custom_env_name = \"onnx-deployment\"\n",
    "\n",
    "pipeline_job_env = Environment(\n",
    "    name=custom_env_name,\n",
    "    description=\"Custom environment for titanic survival pipeline\",\n",
    "    tags={\"onnx-deployment\": \"1.0\"},\n",
    "    conda_file=os.path.join(dependencies_dir, \"conda.yml\"),\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-cuda11.8-cudnn8-ubuntu22.04:latest\",\n",
    "    version=\"1.0\",\n",
    ")\n",
    "pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
    "\n",
    "print(\n",
    "    f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21fdb7d-a6f4-491d-b12d-e3776845177a",
   "metadata": {},
   "source": [
    "# Load Test Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9ce8da6-1c1c-48f8-8d20-087f3c8286ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('P95 latency (ms) - 157.9905882501862; Average latency (ms) - 135.59 +\\\\- 12.27;',\n",
       " 157.9905882501862)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import urllib.request\n",
    "import json\n",
    "import os\n",
    "import ssl\n",
    "from time import perf_counter\n",
    "import numpy as np\n",
    "\n",
    "def allowSelfSignedHttps(allowed):\n",
    "    # bypass the server certificate verification on client side\n",
    "    if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "allowSelfSignedHttps(True) # this line is needed if you use self-signed certificate in your scoring service.\n",
    "\n",
    "\n",
    "def measure_latency(req):\n",
    "    latencies = []\n",
    "    # warm up\n",
    "    for _ in range(10):\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            response = urllib.request.urlopen(req)\n",
    "\n",
    "            result = response.read()\n",
    "            #print(result)\n",
    "        except urllib.error.HTTPError as error:\n",
    "            print(\"The request failed with status code: \" + str(error.code))\n",
    "\n",
    "            # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n",
    "            print(error.info())\n",
    "            print(error.read().decode(\"utf8\", 'ignore'))\n",
    "        \n",
    "    # Timed run\n",
    "    for _ in range(100):\n",
    "        start_time = perf_counter()\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            \n",
    "            response = urllib.request.urlopen(req)\n",
    "\n",
    "            result = response.read()\n",
    "            #print(result)\n",
    "        except urllib.error.HTTPError as error:\n",
    "            print(\"The request failed with status code: \" + str(error.code))\n",
    "\n",
    "            # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n",
    "            print(error.info())\n",
    "            print(error.read().decode(\"utf8\", 'ignore'))\n",
    "\n",
    "        \n",
    "        latency = perf_counter() - start_time\n",
    "        latencies.append(latency)\n",
    "    # Compute run statistics\n",
    "    time_avg_ms = 1000 * np.mean(latencies)\n",
    "    time_std_ms = 1000 * np.std(latencies)\n",
    "    time_p95_ms = 1000 * np.percentile(latencies,95)\n",
    "    return f\"P95 latency (ms) - {time_p95_ms}; Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f};\", time_p95_ms\n",
    "\n",
    "\n",
    "\n",
    "# Request data goes here\n",
    "# The example below assumes JSON formatting which may be updated\n",
    "# depending on the format your endpoint expects.\n",
    "# More information can be found here:\n",
    "# https://docs.microsoft.com/azure/machine-learning/how-to-deploy-advanced-entry-script\n",
    "data = {\"question\" :  \"When did Virgin Australia start operating?\", \"context\": \"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\"}\n",
    "\n",
    "body = str.encode(json.dumps(data))\n",
    "\n",
    "url = 'https://aml-worspace-hldtr.eastus2.inference.ml.azure.com/score'\n",
    "# Replace this with the primary/secondary key or AMLToken for the endpoint\n",
    "api_key = 'EqxpQQHYLQLmxV8u3Tep9nsuQZWFswqC'\n",
    "if not api_key:\n",
    "    raise Exception(\"A key should be provided to invoke the endpoint\")\n",
    "\n",
    "# The azureml-model-deployment header will force the request to go to a specific deployment.\n",
    "# Remove this header to have the request observe the endpoint traffic rules\n",
    "headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key), 'azureml-model-deployment': 'bert-large-uncased-whole-word-1' }\n",
    "\n",
    "req = urllib.request.Request(url, body, headers)\n",
    "\n",
    "measure_latency(req)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b24008-e3b3-4ec5-b601-41d82c2e71a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
